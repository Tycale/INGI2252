% LINGI2252 - Measures & Maintenance
% Code Analysis
\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}


\lstset{%
	basicstyle=\ttfamily\footnotesize,
	commentstyle=\color{green!90!black},
%	frame=single,
	keywordstyle=\bfseries\color{blue},
	language=python,
	numberstyle=\color{gray},
%	tabsize=2,
}


\hypersetup{%
	colorlinks=true,
	linkcolor=blue,
	urlcolor=blue
}


\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}

\newcommand{\pyl}{\textsf{Pylint}}


\title{\tbf{UCL} \\
	LINGI2252 - Measures \& Maintenance}
\author{Group 23 \\
		Thibault Gerondal \& Michael Heraly \\
		\\
		\tit{Teacher: Kim Mens}}
%\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle


\section*{Presentation of Reddit}

Reddit is a popular social networking service that allows its members to share content as text posts or links.
Members can vote ``up'' or ``down'' on a content to promote it to other members.
The content is organized in subcategories, called \tit{subreddits}.
Reddit is an open-source project, and its code is available on Github\footnote{\url{https://github.com/reddit/reddit}}.


\section{Methodology}

After installing Reddit and its dependencies\footnote{\url{https://github.com/reddit/reddit/wiki/Install-guide}}, we have run \pyl{} on the source code.
We used the \tit{parseable} output feature that \pyl{} offers, and created a script to create a CVS file format from the parseable file.
This way, we created a spreadsheet containing the informations about errors/warnings/refactors detected by \pyl{}: the file concerned, at which line, code of the error, and the message associated.
This was a practical help to analyse the \pyl{} output, and compute the statistics.


\section{Type of errors}


\section{Statistics}

\subsection*{Precision}

We computed the precision on all the errors/warnings given by \pyl{}.
We checked the source code to determine if these were true or false-positives.
This way, we can have a good idea of the precision of \pyl{}.

We have computed the precision for each category of the informations given by \pyl{}.


\subsection*{Recall}

As the project is too big to be analysed in details, we have selected 2 files to compute the \tit{recall} statistic.


\section{Bad things}

\hrulefill
\begin{lstlisting}[caption=3 nested for-loops - 2 times in a row - in a single function]
for campaign in all_campaigns:
    camp_dates = set(get_date_range(campaign.start_date, \
					campaign.end_date))
    sr_names = tuple(sorted(campaign.target.subreddit_names))
    daily_impressions = campaign.impressions / campaign.ndays

    for location in locations:
        if location and not location.contains(campaign.location):
            # campaign's location is less specific than location
            continue

        for date in camp_dates.intersection(dates):
            booked_dict[date][location][sr_names] += daily_impressions

    # calculate inventory for each target and location on each date
    datekey = lambda dt: dt.strftime('%m/%d/%Y') if datestr else dt

ret = {}
for target in targets:
    name = make_target_name(target)
    subreddit_names = target.subreddit_names
    ret[name] = {}
    for date in dates:
        pageviews_by_location = {}
        for location in locations:
            # calculate available impressions for each location
            booked_by_target = booked_dict[date][location]
            pageviews_by_sr_name = pageviews_dict[location]
            pageviews_by_location[location] = get_maximized_pageviews(
                subreddit_names, booked_by_target, pageviews_by_sr_name)
        # available pageviews is the minimum from all locations
        min_pageviews = min(pageviews_by_location.values())
        ret[name][datekey(date)] = max(0, min_pageviews)
\end{lstlisting}
\hrulefill


\end{document}
